## INTRODUCTION

  Speech Emotion Recognition (SER) is a task of recognizing the emotion from speech irrespective of the semantic contents. However, emotions are subjective and even for humans it is hard to notate them in neural speech communication regardless of the meaning. The ability to automatically conduct is very difficult task and still an ongoing subject of research. In this project we will use the audio speech files made by 24 actors (12 male and 12 female) vocalizing two lexically-matched statements in a North-American accent. Speech includes eight expressed emotions (neutral, calm, happy, angry , sad, fearful, disgust, and surprise) expressed in normal and strong intensity. In this project, the first step is working with audio files is to turn the audio wave into numbers so that you can feed this into your machine learning algorithm. That will be like as the sound wave that move over time and which can be plotted on the graph. From that graph we can measure the height of the wave at equal intervals This is exactly what the sampling rate means.
  
  The speech emotion recognition using Librosa include loading the audio file, preprocessing the waveform, extracting features from the audio data, training a machine learning model, and making predictions on new audio data. Common features for speech emotion recognition include MFCCs (Mel Frequency Cepstral Coefficient), chroma features, and spectral contrast, while common classifiers include SVMs (Support Vector Machines) and neural networks.
